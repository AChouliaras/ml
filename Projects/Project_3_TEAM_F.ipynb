{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "### Team F\n",
    "\n",
    "Chouliaras Andreas 2143 <br>\n",
    "Pappas Apostolos 2109\n",
    "\n",
    "\\textbf{Part I}<br>\n",
    "\n",
    "\\textbf{1.} To minimize $\\sum_i (x_i - b)^2$, we first need to compute the derivative with respect to b. Thus, we have:\n",
    "\n",
    "$$\\frac{\\partial \\sum_i (x_i - b)^2}{\\partial b} = $$\n",
    "\n",
    "$$ 2\\sum_i (x_i - b)(-1) $$\n",
    "\n",
    "Now, we set the derivative equal to zero and solve the equation with respect to b:\n",
    "\n",
    "$$ 2\\sum_i (x_i - b)(-1) = 0 \\Rightarrow$$\n",
    "\n",
    "$$ - \\sum_i x_i + \\sum_i b = 0 \\Rightarrow$$\n",
    "\n",
    "$$ \\sum_i b = \\sum_i x_i \\Rightarrow$$\n",
    "\n",
    "$$ N b = \\sum_i x_i \\Rightarrow$$\n",
    "\n",
    "$$ b = \\frac{\\sum_i x_i}{N} \\Rightarrow$$\n",
    "\n",
    "$$ b = \\bar{x}$$\n",
    "\n",
    "As we can notice, this means that the optimal b is the mean of x.\n",
    "\n",
    "\\textbf{2.} We first rewrite the problem in matrix and vector notation as follows:\n",
    "\n",
    "$$ L(w) = (\\mathbf{X}w - y)^2$$\n",
    "\n",
    "If we further analyse the above formula, we get:\n",
    "\n",
    "\\begin{aligned}\n",
    " L(w) &= (\\mathbf{X}w - y)^2 = \\\\\n",
    " &= (\\mathbf{X}w - y)^T(\\mathbf{X}w - y) = \\\\\n",
    " &= ((\\mathbf{X}w)^T - y^T)(\\mathbf{X}w - y) = \\\\\n",
    " &= (\\mathbf{X}w)^T\\mathbf{X}w - (\\mathbf{X}w)^Ty - y^T\\mathbf{X}w + y^Ty = \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Let's have a look at our $(\\mathbf{X}w)^Ty$ and $y^T\\mathbf{X}w$ terms. Both of these terms are scalar values. We, then, are able to subtitute $(\\mathbf{X}w)^Ty$ with its transposed form. Thus, since $((\\mathbf{X}w)^Ty)^T = y^T\\mathbf{X}w$, we get:\n",
    "\n",
    "\\begin{aligned}\n",
    " L(w) &= (\\mathbf{X}w)^T\\mathbf{X}w -  2y^T\\mathbf{X}w + y^Ty \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Calculating the gradient of the optimization problem with respect to w,\n",
    "\n",
    "\\begin{aligned}\n",
    " \\frac{\\partial L(w)}{\\partial w} &= X^TXw +X^TXw - 2X^Ty =\\\\\n",
    " &= 2X^TXw -2X^Ty\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Now we equalize $\\frac{\\partial L(w)}{\\partial w}$ to zero and solve with respect to $w$:\n",
    "\n",
    "\\begin{aligned}\n",
    " &\\frac{\\partial L(w)}{\\partial w} = 0 \\Rightarrow\\\\\n",
    " &  2X^TXw -2X^Ty = 0 \\Rightarrow\\\\\n",
    " &  X^TXw = X^Ty \\Rightarrow \\\\\n",
    " &  w = (X^TX)^{-1}X^Ty\n",
    "\\end{aligned}\n",
    "\n",
    "The advantage of the above method is that the closed-form solution is the most mathematicaly correct solution, that may and should be preferred for \"smaller\" datasets. But because of the expensive calculation of the inverse matrix it renders the aformentioned method inapplicable for very \"large\" datasets. This is the reason why, iterative methods like Stohastic Gradient Descend are more commonly used in real applications where the datasets are usually \"large\".\n",
    "\n",
    "For high-dimensional $X$ matrices ,that can happen due to many observations and a lot of features, the computational cost for the $(X^TX)^{-1}$ inverse matrix becomes more and more expensive causing the closed form method to become less and less applicable.\n",
    "\n",
    "\\textbf{3.} Ommiting the bias $b$ like before we have:\n",
    "\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + \\epsilon \\text{ where } \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Solving with respect to $\\epsilon$ we get:\n",
    "$$ \\epsilon = y - \\mathbf{w}^\\top \\mathbf{x} $$\n",
    "\n",
    "Assuming that the noise model governing the additive noise $\\epsilon$ is the exponential distribution we have $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$\n",
    "The likelihood function is:\n",
    "\n",
    "$$p(y|\\mathbf{x}) = \\frac{1}{2} \\prod_{i=1}^{n} \\exp\\left(- |y_i - \\mathbf{w}^\\top \\mathbf{x_i}|\\right)$$\n",
    "\n",
    "Continuing we calculate the log likelihood as:\n",
    "\n",
    "\\begin{aligned}\n",
    "log p(y|\\mathbf{x}) &= \\sum_{i=1}^{n} log 2^{-1} + log \\exp\\left(-|y_i - \\mathbf{w}^\\top \\mathbf{x_i}|\\right) \\\\\n",
    "&= \\sum_{i=1}^{n} -log2 -|y_i - \\mathbf{w}^\\top \\mathbf{x_i}| \n",
    "\\end{aligned}\n",
    "\n",
    "So the negative log likelihood we were asked to find is:\n",
    "\n",
    "$$ -log p(y|\\mathbf{x}) = nlog2 + \\sum_{i=1}^{n} |y_i - \\mathbf{w}^\\top \\mathbf{x_i}|$$\n",
    "\n",
    "To find the closed form solution we diferenciate the above formula with respect to $w$\n",
    "$$\n",
    "\\frac{\\partial }{\\partial w} (-log p(y|\\mathbf{x}))= \\sum_{i=1}^{n} \\frac{\\partial }{\\partial w}|y_i - \\mathbf{w}^\\top \\mathbf{x_i}|\n",
    "$$\n",
    "\n",
    "It gives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial }{\\partial w} (-log p(y|\\mathbf{x}))= \\sum_{i=1}^{n} g(y_i,x_i,w)\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "g(y_i,x_i,w) = \n",
    "\\begin{cases}\n",
    "    x_i^T   &\\quad\\text{,if } y_i \\ge \\mathbf{w}^\\top \\mathbf{x_i} \\\\\n",
    "    -x_i^T  &\\quad\\text{,if } y_i < \\mathbf{w}^\\top \\mathbf{x_i}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\\textbf{Part IΙ}\n",
    "\n",
    "\\textbf{1.} Initializing our weights to zero might break our algorithm. The problem with initializing our weights to zero is that the algorithm might fail to make any changes to the weights and the model will stuck. Instead, a better idea is to initialize the weights to random values close to zero. For example, at Project 3 - Part II - linear-regression-scratch the weights are initialized with random normal values having zero mean and a standard deviation of 0.01.\n",
    "\n",
    "\\textbf{2.} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T18:13:12.597091Z",
     "start_time": "2019-03-24T18:13:12.388276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1065.522217\n",
      "epoch 2, loss 39.419380\n",
      "epoch 3, loss 1.957459\n",
      "epoch 4, loss 0.671257\n",
      "epoch 5, loss 0.614576\n",
      "epoch 6, loss 0.612986\n",
      "epoch 7, loss 0.611622\n",
      "epoch 8, loss 0.611392\n",
      "epoch 9, loss 0.611154\n",
      "epoch 10, loss 0.610440\n",
      "Estimated R: \n",
      "[[42.057808]]\n",
      "<NDArray 1x1 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "num_inputs = 1\n",
    "num_examples = 20\n",
    "\n",
    "current = nd.array([1.5420291, 1.8935232, 2.1603365, 2.5381863, 2.893443, \\\n",
    "                    3.838855, 3.925425, 4.2233696, 4.235571, 4.273397, \\\n",
    "                    4.9332876, 6.4704757, 6.517571, 6.87826, 7.0009003, \\\n",
    "                    7.035741, 7.278681, 7.7561755, 9.121138, 9.728281])\n",
    "\n",
    "voltage = nd.array([63.802246, 80.036026, 91.4903, 108.28776, 122.781975, \\\n",
    "                    161.36314, 166.50816, 176.16772, 180.29395, 179.09758, \\\n",
    "                    206.21027, 272.71857, 272.24033, 289.54745, 293.8488, \\\n",
    "                    295.2281, 306.62274, 327.93243, 383.16296, 408.65967])\n",
    "features=current.reshape(num_examples, num_inputs)\n",
    "labels=voltage\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features.take(j), labels.take(j)\n",
    "        # The “take” function will then return the corresponding element based\n",
    "        # on the indices\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))\n",
    "\n",
    "w.attach_grad()\n",
    "\n",
    "def linreg(X, w):\n",
    "    return nd.dot(X, w)\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "\n",
    "lr = 0.01  # Learning rate\n",
    "num_epochs = 10  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w,b]\n",
    "        sgd([w], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))\n",
    "    \n",
    "\n",
    "print('Estimated R:', w)\n",
    "\n",
    "estimate=  w[0] * current\n",
    "\n",
    "#see how the models do\n",
    "fig = plt.figure()\n",
    "plt.plot(current.asnumpy(), voltage.asnumpy(), label='data')\n",
    "plt.plot(current.asnumpy(), estimate.asnumpy(), label='prediction')\n",
    "fig.suptitle('Ohm model')\n",
    "plt.xlabel('current')\n",
    "plt.ylabel('voltage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{3.} Extending `autograd` to second derivatives creates some problems. The `autograd` library doesn't have a distinct function to calculate the second derivative with respect to each element seperately. So to overcome this problem two things can be done. \n",
    "\n",
    "The first is to calculate the Hessian Matrix and extract the diagonal values which contain the second derivatives we need. But this way involves a lot of unnecessary computations that in the end are discarded. \n",
    "\n",
    "The other way is to calculate the full gradient as before and then, picking out the first element and taking the gradient of that, then picking out the second element and taking the gradient of that etc.\n",
    "\n",
    "\\textbf{4.} In the `squared_loss` function we need the `reshape` function to make the two arrays have the same dimensions in order to subtract them. The y_hat array has (10,1) size while the y array has (10,) size. After the reshape they both have (10,1) size and we are able to make the subtraction.\n",
    "\n",
    "\\textbf{5.} After experimenting with various values for the learning rate we noticed:\n",
    "\n",
    "Decreasing the learning rate any further from the initial 0.03 the loss function drops with a slower rate. In fact the loss function needs a lot more iterations to reach the minimum loss value.\n",
    "\n",
    "On the other hand, increasing the learning rate makes a major impact to the loss function. Not only it drops faster but it \n",
    "also overshoots the minimum in the most cases. It becomes greatly unstable as it tries to converge. \n",
    "\n",
    "\\textbf{6.} If the number of examples cannot be exactly divided by the batch size, this means that the final examples, which are less than batch size, will be trained to the model but they will have a little more impact to the parameters. This means we will get to have a bit different results in performance, than using right batch size. This difference depends firstly on the size of the dataset and secondly on the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
