{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-basics\"></a>\n",
    "# Linear regression basics\n",
    "---\n",
    "\n",
    "There are two variations of the Linear Regression.  Simple Linear Regression and Multi-Variable Linear Regression.\n",
    "\n",
    "Both equations have a dependent variable, often denoted as $y$, independent variable(s) often denoted at $x$ and a constant commonly referred to as the y intercept.    \n",
    "_Simple linear regression has one independent, multi linear has mutliple independent variables. _\n",
    "\n",
    "## Form of linear regression\n",
    "\n",
    "Simple LR uses one feature and a constant to represent a relationship with another feature.\n",
    "## $y = \\alpha + \\beta X +\\epsilon_i $ or (more commonly) $ y = mx + b$\n",
    "\n",
    "Multi-Variable LR uses 2-to-infinite features and a constant to represent a relationship with another feature.\n",
    "\n",
    "## $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon_i$\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "- $\\epsilon_i$ is the constant error\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- And once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](estimating_coefficients.png)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use a linear regression?\n",
    "**Improve our understanding of our data and summarize the information**\n",
    "- What is the relation that out dependent variable(s) have with the independent?  \n",
    "\n",
    "**Make Predictions**\n",
    "- Utilize the relation we found to make speculations.\n",
    "- What is a dependent variable likely to be, given an independent or combination of independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is a Linear Regression Calculated?\n",
    "*How do we find the blue line*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we minimize the sum of squared errors? As usual, we find the minimum of a function by taking the derivative and setting it to zero. This is a little involved, but not too difficult. The sum of squared errors is written as $E$\n",
    "\n",
    "$$\n",
    "E=\\sum_{i=1}^N\\varepsilon_i^2=\n",
    "\\sum_{i=1}^N[y_i-(ax_i+b)]^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of observations. The slope $a$ and intercept $b$ are determined such that $E$ is minimized, which means that the following derivatives are zero\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial a}=0 \\qquad \\frac{\\partial E}{\\partial b}=0$$\n",
    "\n",
    "Differentiation gives (using the chain rule)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial a}=\\sum_{i=1}^N[2(y_i-ax_i-b)(-x_i)]=\n",
    "2a\\sum_{i=1}^Nx_i^2+2b\\sum_{i=1}^Nx_i-2\\sum_{i=1}^Nx_iy_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b}=\\sum_{i=1}^N[2(y_i-ax_i-b)(-1)]=\n",
    "2a\\sum_{i=1}^Nx_i+2bN-2\\sum_{i=1}^Ny_i\n",
    "$$\n",
    "\n",
    "Setting the derivatives equal to zero and division by 2 gives\n",
    "\n",
    "$$\n",
    "a\\sum_{i=1}^Nx_i^2+b\\sum_{i=1}^Nx_i-\\sum_{i=1}^Nx_iy_i=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "a\\sum_{i=1}^Nx_i+bN-\\sum_{i=1}^Ny_i=0\n",
    "$$\n",
    "\n",
    "This system of two linear equations with two unknowns ($a$ and $b$) may be solved to give\n",
    "\n",
    "$$ a=\\frac{N\\sum_{i=1}^Nx_iy_i-\\sum_{i=1}^Nx_i\\sum_{i=1}^Ny_i}\n",
    "{N\\sum_{i=1}^Nx_i^2-\\sum_{i=1}^Nx_i\\sum_{i=1}^Nx_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b=\\bar{y}-a\\bar{x}\n",
    "$$\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean values of $x$ and $y$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "The covariance of two variables x and y in a data set measures how the two are linearly related. A positive covariance would indicate a positive linear relationship between the variables, and a negative covariance would indicate the opposite.\n",
    "\n",
    "The sample covariance is defined in terms of the sample means as:\n",
    "\n",
    "$Cov_{x,y}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}  \\equiv s_{xy}$\n",
    "\n",
    "\n",
    "Similarly, the population covariance is defined in terms of the population mean $\\mu_x$, $\\mu_y$ as:\n",
    "\n",
    "$Cov_{x,y}=\\frac{\\sum_{i=1}^{N}(x_{i}-\\mu_x)(y_{i}-\\mu_y)}{N} \\equiv \\sigma_{xy}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "The variance is a numerical measure of how the data values is dispersed around the mean. In particular, the sample variance is defined as:\n",
    "\n",
    "   $$s^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x}_i)^2} {n-1}  \\equiv Var(x)$$\n",
    "         \n",
    "\n",
    "Similarly, the population variance is defined in terms of the population mean μ and population size N:\n",
    "\n",
    "  $$\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2} {N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-lr\"></a>\n",
    "### How is a Linear Regression Calculated?\n",
    "*How do we find the blue line*\n",
    "\n",
    "**Simple Linear Regression**\n",
    "- In a simple linear regression the line of \"best fit\" is found using the Least Squares Method which is intended to minimize the sum of squares of the residuals.  \n",
    "\n",
    "In the terms of the the simple linear regression if is found by taking the quotient of the Covariance and the variance. (Said result is then applied to the means of $x$ and $y$ to find the appropriate constant value.)\n",
    "\n",
    "### $ α = \\frac{Cov(x,y)}{Var(x)} $\n",
    "\n",
    "### $ b = \\bar{y} - (α*\\bar{x})$\n",
    "\n",
    "**Multi Linear Regression**\n",
    "- Multi Linear Regression still lies on calculations using Variance and Covariance, however because it needs to be done across multiple dimensions we need to use a linear algebra approach.\n",
    "\n",
    "$$\\beta = (X'X)^{-1}X'y$$\n",
    "\n",
    "- $X$ = Our Features  \n",
    "- $X'$ = $X$ Transposed\n",
    "- $y$ = Our Target\n",
    "\n",
    "If we break some of the matrix down we can see what their results are.\n",
    "\n",
    "$$ X'X = \\begin{bmatrix}\n",
    "    var(x_1) & cov(x_1, x_2) & cov(x_1, x_3)  \\\\\n",
    "    cov(x_2, x_1) & var(x_2) & cov(x_2, x_3) \\\\\n",
    "    cov(x_3, x_1) & cov(x_3, x_2) & var(x_3)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$X'X$ is pretty much a matrix of all the variable variance and covariance combinations.\n",
    "\n",
    "$$X'y = \\begin{bmatrix}\n",
    "    cov(y, x_1) \\\\\n",
    "    cov(y, x_2) \\\\\n",
    "    cov(y, x_3) )\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$X'y$ is a metric of all the individual features and $y$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression - Example\n",
    "=================\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "Linear Regression is a supervised machine learning algorithm where the\n",
    "predicted output is continuous and has a constant slope. Is used to\n",
    "predict values within a continuous range. (e.g. sales, price) rather\n",
    "than trying to classify them into categories (e.g. cat, dog). There are\n",
    "two main types:\n",
    "\n",
    "**Simple regression**\n",
    "\n",
    "Simple linear regression uses traditional slope-intercept form, where\n",
    "$m$ and $b$ are the variables our algorithm will try to \"learn\" to\n",
    "produce the most accurate predictions. $x$ represents our input data and\n",
    "$y$ represents our prediction.\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "**Multivariable regression**\n",
    "\n",
    "A more complex, multi-variable linear equation might look like this,\n",
    "where $w$ represents the coefficients, or weights, our model will try to\n",
    "learn.\n",
    "\n",
    "$$f(x,y,z) = w_1 x + w_2 y + w_3 z$$\n",
    "\n",
    "The variables $x, y, z$ represent the attributes, or distinct pieces of\n",
    "information, we have about each observation. For sales predictions,\n",
    "these attributes might include a company's advertising spend on radio,\n",
    "TV, and newspapers.\n",
    "\n",
    "$$Sales = w_1 Radio + w_2 TV + w_3 News$$\n",
    "\n",
    "Simple regression\n",
    "-----------------\n",
    "\n",
    "Let’s say we are given a\n",
    "[dataset](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) with the\n",
    "following columns (features): how much a company spends on Radio\n",
    "advertising each year and its annual Sales in terms of units sold. We\n",
    "are trying to develop an equation that will let us to predict units sold\n",
    "based on how much a company spends on radio advertising. The rows\n",
    "(observations) represent companies.\n",
    "\n",
    "  ---------------- ----------------- -------------\n",
    "  **Company**      **Radio (\\$)**    **Sales**\n",
    "  Amazon           37.8              22.1\n",
    "  Google           39.3              10.4\n",
    "  Facebook         45.9              18.3\n",
    "  Apple            41.3              18.5\n",
    "  ---------------- ----------------- -------------\n",
    "\n",
    "### Making predictions\n",
    "\n",
    "Our prediction function outputs an estimate of sales given a company's\n",
    "radio advertising spend and our current values for *Weight* and *Bias*.\n",
    "\n",
    "$$Sales = Weight \\cdot Radio + Bias$$\n",
    "\n",
    "Weight\n",
    "\n",
    ":   the coefficient for the Radio independent variable. In machine\n",
    "    learning we call coefficients *weights*.\n",
    "\n",
    "Radio\n",
    "\n",
    ":   the independent variable. In machine learning we call these\n",
    "    variables *features*.\n",
    "\n",
    "Bias\n",
    "\n",
    ":   the intercept where our line intercepts the y-axis. In machine\n",
    "    learning we can call intercepts *bias*. Bias offsets all predictions\n",
    "    that we make.\n",
    "\n",
    "Our algorithm will try to *learn* the correct values for Weight and\n",
    "Bias. By the end of our training, our equation will approximate the\n",
    "*line of best fit*.\n",
    "\n",
    "![image](images/linear_regression_line_intro.png)\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:23:04.657965Z",
     "start_time": "2019-02-20T12:23:04.649965Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sales(radio, weight, bias):\n",
    "    return weight*radio + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "The prediction function is nice, but for our purposes we don't really\n",
    "need it. What we need is a cost function &lt;loss\\_functions&gt; so we\n",
    "can start optimizing our weights.\n",
    "\n",
    "Let's use mse as our cost function. MSE measures the average squared\n",
    "difference between an observation's actual and predicted values. The\n",
    "output is a single number representing the cost, or score, associated\n",
    "with our current set of weights. Our goal is to minimize MSE to improve\n",
    "the accuracy of our model.\n",
    "\n",
    "**Math**\n",
    "\n",
    "Given our simple linear equation $y = mx + b$, we can calculate MSE as:\n",
    "\n",
    "$$MSE =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (m x_i + b))^2$$\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "\n",
    "-   $N$ is the total number of observations (data points)\n",
    "-   $\\frac{1}{N} \\sum_{i=1}^{n}$ is the mean\n",
    "-   $y_i$ is the actual value of an observation and $m x_i + b$ is our\n",
    "    prediction\n",
    "\n",
    "</div>\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:23:13.391362Z",
     "start_time": "2019-02-20T12:23:13.383362Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(radio, sales, weight, bias):\n",
    "    \n",
    "    total_error = 0.0\n",
    "    for i in range(companies):\n",
    "        total_error += (sales[i] - (weight*radio[i] + bias))**2\n",
    "    return total_error / companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "To minimize MSE we use gradient\\_descent to calculate the gradient of\n",
    "our cost function.\n",
    "\n",
    "**Math**\n",
    "\n",
    "There are two parameters &lt;glossary\\_parameters&gt; (coefficients) in\n",
    "our cost function we can control: weight $m$ and bias $b$. Since we need\n",
    "to consider the impact each one has on the final prediction, we use\n",
    "partial derivatives. To find the partial derivatives, we use the\n",
    "chain\\_rule. We need the chain rule because $(y - (mx + b))^2$ is really\n",
    "2 nested functions: the inner function $y - mx + b$ and the outer\n",
    "function $x^2$.\n",
    "\n",
    "Returning to our cost function:\n",
    "\n",
    "$$f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2$$\n",
    "\n",
    "We can calculate the gradient of this cost function as:\n",
    "\n",
    "$$f'(m,b) =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{df}{dm}\\\\\n",
    "     \\frac{df}{db}\\\\\n",
    "    \\end{bmatrix}\n",
    "=\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n",
    "     \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n",
    "    \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "**gradient descent algorith**\n",
    "\n",
    "**until** stopping criteria:\n",
    "\n",
    "    1. Evaluate gradients at each data point in the dataset \n",
    "    2. update parameters of the function to approximate the data \n",
    "$$m = m - \\alpha \\frac{df}{dm}$$\n",
    "$$b = b - \\alpha \\frac{df}{db}$$\n",
    "\n",
    "Where $\\alpha$ is called learning rate and relates to much we trust the gradient at a given point, it is usually the case that $0< \\alpha <1$. Setting the learning rate too high might lead to divergence since it risks overshooting the minimum, as illustrated below.\n",
    "\n",
    "\n",
    "To solve for the gradient, we iterate through our data points using our\n",
    "new weight and bias values and take the average of the partial\n",
    "derivatives. The resulting gradient tells us the slope of our cost\n",
    "function at our current position (i.e. weight and bias) and the\n",
    "direction we should update to reduce our cost function (we move in the\n",
    "direction opposite the gradient). The size of our update is controlled\n",
    "by the learning rate.\n",
    "\n",
    "![grad1](sgd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the learning rate too high might lead to divergence since it risks overshooting the minimum, as illustrated below.\n",
    "![divergence](diverging.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:23:22.186258Z",
     "start_time": "2019-02-20T12:23:22.155014Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_weights(radio, sales, weight, bias, learning_rate):\n",
    "    weight_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    companies = len(radio)\n",
    "\n",
    "    for i in range(companies):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        weight_deriv += -2*radio[i] * (sales[i] - (weight*radio[i] + bias))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        bias_deriv += -2*(sales[i] - (weight*radio[i] + bias))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    weight -= (weight_deriv / companies) * learning_rate\n",
    "    bias -= (bias_deriv / companies) * learning_rate\n",
    "\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many implementations of Gradient Descent, with the explained above being the simplest (and slowest) among them.  The following figure illustrates  various algorithms and their convergence.\n",
    "\n",
    "![alorithms](gds.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training a model is the process of iteratively improving your prediction\n",
    "equation by looping through the dataset multiple times, each time\n",
    "updating the weight and bias values in the direction indicated by the\n",
    "slope of the cost function (gradient). Training is complete when we\n",
    "reach an acceptable error threshold, or when subsequent training\n",
    "iterations fail to reduce our cost.\n",
    "\n",
    "Before training we need to initializing our weights (set default\n",
    "values), set our hyperparameters &lt;glossary\\_hyperparameters&gt;\n",
    "(learning rate and number of iterations), and prepare to log our\n",
    "progress over each iteration.\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:23:34.993894Z",
     "start_time": "2019-02-20T12:23:34.947026Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(radio, sales, weight, bias, learning_rate, iters):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weight,bias = update_weights(radio, sales, weight, bias, learning_rate)\n",
    "\n",
    "        #Calculate cost for auditing purposes\n",
    "        cost = cost_function(features, targets, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if i % 10 == 0:\n",
    "            print (\"iter: \"+str(i) + \" cost: \"+str(cost))\n",
    "\n",
    "    return weight, bias, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Apply the code to the given Advertisement dataset and produce the following output\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "If our model is working, we should see our cost decrease after every\n",
    "iteration.\n",
    "\n",
    "**Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T07:06:19.704906Z",
     "start_time": "2019-02-20T07:06:19.696906Z"
    }
   },
   "outputs": [],
   "source": [
    "iter=1     weight=.03    bias=.0014    cost=197.25\n",
    "iter=10    weight=.28    bias=.0116    cost=74.65\n",
    "iter=20    weight=.39    bias=.0177    cost=49.48\n",
    "iter=30    weight=.44    bias=.0219    cost=44.31\n",
    "iter=30    weight=.46    bias=.0249    cost=43.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing**\n",
    "\n",
    "![image](images/linear_regression_line_1.png)\n",
    "\n",
    "![image](images/linear_regression_line_2.png)\n",
    "\n",
    "![image](images/linear_regression_line_3.png)\n",
    "\n",
    "![image](images/linear_regression_line_4.png)\n",
    "\n",
    "**Cost history**\n",
    "\n",
    "![image](images/linear_regression_training_cost.png)\n",
    "\n",
    "### Summary\n",
    "\n",
    "By learning the best values for weight (.46) and bias (.25), we now have\n",
    "an equation that predicts future sales based on radio advertising\n",
    "investment.\n",
    "\n",
    "$$Sales = .46 Radio + .025$$\n",
    "\n",
    "How would our model perform in the real world? I’ll let you think about\n",
    "it :)\n",
    "\n",
    "Multivariable regression\n",
    "------------------------\n",
    "\n",
    "Let’s say we are given\n",
    "[data](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) on TV, radio,\n",
    "and newspaper advertising spend for a list of companies, and our goal is\n",
    "to predict sales in terms of units sold.\n",
    "\n",
    "  --------------- ----------- ----------- ---------- -----------\n",
    "  Company         TV          Radio       News       Units\n",
    "  Amazon          230.1       37.8        69.1       22.1\n",
    "  Google          44.5        39.3        23.1       10.4\n",
    "  Facebook        17.2        45.9        34.7       18.3\n",
    "  Apple           151.5       41.3        13.2       18.5\n",
    "  --------------- ----------- ----------- ---------- -----------\n",
    "\n",
    "### Growing complexity\n",
    "\n",
    "As the number of features grows, the complexity of our model increases\n",
    "and it becomes increasingly difficult to visualize, or even comprehend,\n",
    "our data.\n",
    "\n",
    "![image](images/linear_regression_3d_plane_mlr.png)\n",
    "\n",
    "One solution is to break the data apart and compare 1-2 features at a\n",
    "time. In this example we explore how Radio and TV investment impacts\n",
    "Sales.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "As the number of features grows, calculating gradient takes longer to\n",
    "compute. We can speed this up by \"normalizing\" our input data to ensure\n",
    "all values are within the same range. This is especially important for\n",
    "datasets with high standard deviations or differences in the ranges of\n",
    "the attributes. Our goal now will be to normalize our features so they\n",
    "are all in the range -1 to 1.\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For each feature column {\n",
    "    #1 Subtract the mean of the column (mean normalization)\n",
    "    #2 Divide by the range of the column (feature scaling)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is a 200 x 3 matrix containing TV, Radio, and Newspaper data.\n",
    "Our output is a normalized matrix of the same shape with all values\n",
    "between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:24:09.827906Z",
     "start_time": "2019-02-20T12:24:09.796663Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(features):\n",
    "    '''\n",
    "    features     -   (200, 3)\n",
    "    features.T   -   (3, 200)\n",
    "\n",
    "    We transpose the input matrix, swapping\n",
    "    cols and rows to make vector math easier\n",
    "    '''\n",
    "\n",
    "    for feature in features.T:\n",
    "        fmean = np.mean(feature)\n",
    "        frange = np.amax(feature) - np.amin(feature)\n",
    "\n",
    "        #Vector Subtraction\n",
    "        feature -= fmean\n",
    "\n",
    "        #Vector Division\n",
    "        feature /= frange\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "\n",
    "**Matrix math**. Before we continue, it's important to understand basic\n",
    "linear\\_algebra concepts as well as numpy functions like\n",
    "[numpy.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html).\n",
    "\n",
    "</div>\n",
    "\n",
    "### Making predictions\n",
    "\n",
    "Our predict function outputs an estimate of sales given our current\n",
    "weights (coefficients) and a company's TV, radio, and newspaper spend.\n",
    "Our model will try to identify weight values that most reduce our cost\n",
    "function.\n",
    "\n",
    "$$Sales = W_1 TV + W_2 Radio + W_3 Newspaper$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:25:19.254864Z",
     "start_time": "2019-02-20T12:25:19.223617Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "  '''\n",
    "  features - (200, 3)\n",
    "  weights - (3, 1)\n",
    "  predictions - (200,1)\n",
    "  '''\n",
    "  return np.dot(features,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:25:07.894568Z",
     "start_time": "2019-02-20T12:25:07.863323Z"
    }
   },
   "outputs": [],
   "source": [
    "W1 = 0.0\n",
    "W2 = 0.0\n",
    "W3 = 0.0\n",
    "weights = np.array([\n",
    "    [W1],\n",
    "    [W2],\n",
    "    [W3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Now we need a cost function to audit how our model is performing. The\n",
    "math is the same, except we swap the $mx + b$ expression for\n",
    "$W_1 x_1 + W_2 x_2 + W_3 x_3$. We also divide the expression by 2 to\n",
    "make derivative calculations simpler.\n",
    "\n",
    "$$MSE =  \\frac{1}{2N} \\sum_{i=1}^{n} (y_i - (W_1 x_1 + W_2 x_2 + W_3 x_3))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:25:30.610967Z",
     "start_time": "2019-02-20T12:25:30.595344Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(features, targets, weights):\n",
    "    \n",
    "    #Features:(200,3)\n",
    "    #Targets: (200,1)\n",
    "    #Weights:(3,1)\n",
    "    #Returns 1D matrix of predictions\n",
    "    \n",
    "    N = len(targets)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    # Matrix math lets use do this without looping\n",
    "    sq_error = (predictions - targets)**2\n",
    "\n",
    "    # Return average squared error among predictions\n",
    "    return 1.0/(2*N) * sq_error.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Again using the chain\\_rule we can compute the gradient--a vector of\n",
    "partial derivatives describing the slope of the cost function for each\n",
    "weight.\n",
    "\n",
    "\n",
    "$$\n",
    "f'(W_1) = -x_1(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\\\\n",
    "f'(W_2) = -x_2(y - (W_1 x_1 + W_2 x_2 + W_3 x_3)) \\\\\n",
    "f'(W_3) = -x_3(y - (W_1 x_1 + W_2 x_2 + W_3 x_3))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:25:40.680952Z",
     "start_time": "2019-02-20T12:25:40.634084Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_weights(features, targets, weights, lr):\n",
    "    '''\n",
    "    Features:(200, 3)\n",
    "    Targets: (200, 1)\n",
    "    Weights:(3, 1)\n",
    "    '''\n",
    "    x1,x2,x3 = \"global\"\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #Extract our features\n",
    "    x1 = features[:,0]\n",
    "    x2 = features[:,1]\n",
    "    x3 = features[:,2]\n",
    "\n",
    "    # Use matrix cross product (*) to simultaneously\n",
    "    # calculate the derivative for each weight\n",
    "    d_w1 = -x1*(targets - predictions)\n",
    "    d_w2 = -x2*(targets - predictions)\n",
    "    d_w3 = -x3*(targets - predictions)\n",
    "\n",
    "    # Multiply the mean derivative by the learning rate\n",
    "    # and subtract from our weights (remember gradient points in direction of steepest ASCENT)\n",
    "    weights[0][0] -= (lr * np.mean(d_w1))\n",
    "    weights[1][0] -= (lr * np.mean(d_w2))\n",
    "    weights[2][0] -= (lr * np.mean(d_w3))\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Multivariate linear regression.\n",
    "\n",
    "### Simplifying with matrices\n",
    "\n",
    "The gradient descent code above has a lot of duplication. Can we improve\n",
    "it somehow? One way to refactor would be to loop through our features\n",
    "and weights--allowing our function handle any number of features.\n",
    "However there is another even better technique: *vectorized gradient\n",
    "descent*.\n",
    "\n",
    "**Math**\n",
    "\n",
    "We use the same formula as above, but instead of operating on a single\n",
    "feature at a time, we use matrix multiplication to operative on all\n",
    "features and weights simultaneously. We replace the $x_i$ terms with a\n",
    "single feature matrix $X$.\n",
    "\n",
    "$$gradient = -X(targets - predictions)$$\n",
    "\n",
    "**Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:25:57.118943Z",
     "start_time": "2019-02-20T12:25:57.103321Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "X = [\n",
    "    [x1, x2, x3]\n",
    "    [x1, x2, x3]\n",
    "    [x1, x2, x3]\n",
    "]\n",
    "\n",
    "targets = [\n",
    "    [1],\n",
    "    [2],\n",
    "    [3]\n",
    "]\n",
    "'''\n",
    "def update_weights_vectorized(X, targets, weights, lr):\n",
    "    '''\n",
    "    gradient = X.T * (predictions - targets) / N\n",
    "    X: (200, 3)\n",
    "    Targets: (200, 1)\n",
    "    Weights: (3, 1)\n",
    "    '''\n",
    "    companies = len(X)\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(X, weights)\n",
    "\n",
    "    #2 - Calculate error/loss\n",
    "    error = targets - predictions\n",
    "\n",
    "    #3 Transpose features from (200, 3) to (3, 200)\n",
    "    # So we can multiply w the (200,1)  error matrix.\n",
    "    # Returns a (3,1) matrix holding 3 partial derivatives --\n",
    "    # one for each feature -- representing the aggregate\n",
    "    # slope of the cost function across all observations\n",
    "    gradient = np.dot(-X.T,  error)\n",
    "\n",
    "    #4 Take the average error derivative for each feature\n",
    "    gradient /= companies\n",
    "\n",
    "    #5 - Multiply the gradient by our learning rate\n",
    "    gradient *= lr\n",
    "\n",
    "    #6 - Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias term\n",
    "\n",
    "Our train function is the same as for simple linear regression, however\n",
    "we're going to make one final tweak before running: add a\n",
    "bias term &lt;glossary\\_bias\\_term&gt; to our feature matrix.\n",
    "\n",
    "In our example, it's very unlikely that sales would be zero if companies\n",
    "stopped advertising. Possible reasons for this might include past\n",
    "advertising, existing customer relationships, retail locations, and\n",
    "salespeople. A bias term will help us capture this base case.\n",
    "\n",
    "**Code**\n",
    "\n",
    "Below we add a constant 1 to our features matrix. By setting this value\n",
    "to 1, it turns our bias term into a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T12:22:20.434258Z",
     "start_time": "2019-02-20T12:22:20.324906Z"
    }
   },
   "outputs": [],
   "source": [
    "bias = np.ones(shape=(len(features),1))\n",
    "features = np.append(bias, features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{exercise}\n",
    "Apply the above code and produce the following output\n",
    "\n",
    "\\end{exercise}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "After training our model through 1000 iterations with a learning rate of\n",
    ".0005, we finally arrive at a set of weights we can use to make\n",
    "predictions:\n",
    "\n",
    "$$Sales = 4.7TV + 3.5Radio + .81Newspaper + 13.9$$\n",
    "\n",
    "Our MSE cost dropped from 110.86 to 6.25.\n",
    "\n",
    "![image](images/multiple_regression_error_history.png)\n",
    "\n",
    "**References**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
